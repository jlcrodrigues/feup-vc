{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Warping and Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "imagesDir = 'Images_03a' # Change this, according to your images' directory path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/jose.l.rodrigues/.local/lib/python3.10/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "# Read image\n",
    "img = cv2.imread(os.path.join(imagesDir, 'giraffe.jpg')) # Change this, according to your image's path\n",
    "\n",
    "# Resize image to facilitate visualization\n",
    "img = cv2.resize(img, (0, 0), fx = 0.4, fy = 0.4)\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affine Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select original coordinates of three points of the original image\n",
    "ori_coord = np.array([[0, 0], [img.shape[1] - 1, 0], [0, img.shape[0] - 1]]).astype(np.float32)\n",
    "\n",
    "# Select target coordinates (where the points will move to)\n",
    "tar_coord = np.array([[0, img.shape[1]*0.33], [img.shape[1]*0.85, img.shape[0]*0.25], [img.shape[1]*0.15, img.shape[0]*0.7]]).astype(np.float32)\n",
    "\n",
    "# Get affine transformation matrix\n",
    "warp_mat = cv2.getAffineTransform(ori_coord, tar_coord)\n",
    "\n",
    "# Apply transformation to the image\n",
    "warp_dst = cv2.warpAffine(img, warp_mat, (img.shape[1], img.shape[0]))\n",
    "\n",
    "# Show Image\n",
    "cv2.imshow('Warped Image', warp_dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.1: Rotate an image by [defining rotation matrix](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#gafbbc470ce83812914a70abfb604f4326) and then applying transformation to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = 45\n",
    "scale = 1\n",
    "rot_mat = cv2.getRotationMatrix2D((img.shape[1]/2, img.shape[0]/2), angle, scale)\n",
    "\n",
    "# Apply transformation to the image\n",
    "warp_dst = cv2.warpAffine(img, rot_mat, (img.shape[1], img.shape[0]))\n",
    "\n",
    "# Show Image\n",
    "cv2.imshow('Warped Image', warp_dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.2: Rotate an image through an Affine Transformation instead of a rotation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = 30\n",
    "\n",
    "# Select original coordinates of three points of the original image\n",
    "ori_coord = np.array([[0, 0], [img.shape[1] - 1, 0], [0, img.shape[0] - 1]]).astype(np.float32)\n",
    "\n",
    "# Convert angle to radians\n",
    "angle_rad = np.radians(angle)\n",
    "\n",
    "# Calculate rotation matrix\n",
    "rot_mat = np.array([[np.cos(angle_rad), np.sin(angle_rad)],\n",
    "                    [-np.sin(angle_rad), np.cos(angle_rad)]])\n",
    "\n",
    "# Apply rotation to each point in ori_coord \n",
    "tar_coord = np.dot(rot_mat, ori_coord.T).T\n",
    "# Add the center\n",
    "tar_coord = tar_coord + np.array([-img.shape[0]/2, img.shape[1]/2])\n",
    "tar_coord = tar_coord.astype(np.float32)\n",
    "\n",
    "# Get affine transformation matrix\n",
    "warp_mat = cv2.getAffineTransform(ori_coord, tar_coord)  # Use only two points for Affine transformation\n",
    "\n",
    "# Apply transformation to the image\n",
    "warp_dst = cv2.warpAffine(img, warp_mat, (img.shape[1], img.shape[0]))\n",
    "\n",
    "# Show Image\n",
    "cv2.imshow('Warped Image', warp_dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.3: Apply a translation of 100 pixels to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select original coordinates of three points of the original image\n",
    "ori_coord = np.array([[0, 0], [img.shape[1] - 1, 0], [0, img.shape[0] - 1]]).astype(np.float32)\n",
    "\n",
    "# Select target coordinates (where the points will move to)\n",
    "tx = 100\n",
    "tar_coord = np.array([[0 + tx, 0], [img.shape[1] + tx, 0], [tx, img.shape[0]]]).astype(np.float32)\n",
    "\n",
    "# Get affine transformation matrix\n",
    "warp_mat = cv2.getAffineTransform(ori_coord, tar_coord)\n",
    "\n",
    "# Apply transformation to the image\n",
    "warp_dst = cv2.warpAffine(img, warp_mat, (img.shape[1], img.shape[0]))\n",
    "\n",
    "# Show Image\n",
    "cv2.imshow('Warped Image', warp_dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homography\n",
    "\n",
    "Example of homography using feature matching from last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "img1 = cv2.imread(os.path.join(imagesDir, 'match_box01a_1.png'), cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread(os.path.join(imagesDir, 'match_box01a_2.png'), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "cv2.imshow('Query', img1)\n",
    "cv2.imshow('Train', img2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "# Apply FLAN Matcher\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks = 50)\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "# Store all the good matches as per Lowe's ratio test\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "# Stop if no matches are found\n",
    "if len(good) < 10:\n",
    "    print( \"Error: Insufficient number of matches\")\n",
    "    exit(-1)\n",
    "\n",
    "# Obtain points corresponding to the matches in the query and train images\n",
    "query_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "train_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "\n",
    "# Obtain homography that represents the transformation from the points of the train image into the position of the query image \n",
    "M, mask = cv2.findHomography(train_pts, query_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Apply transformation to image\n",
    "warped_img = cv2.warpPerspective(img2, M, (img1.shape[1], img1.shape[0]),flags=cv2.INTER_LINEAR)\n",
    "\n",
    "cv2.imshow('Query', img1)\n",
    "cv2.imshow('Original Image', img2)\n",
    "cv2.imshow('Warped Image', warped_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.4: Draw lines around the object by:\n",
    "* Obtaining homography that transforms points from the query image to the train image\n",
    "* Applying [the perspectiveTransform function](https://docs.opencv.org/3.4/d2/de8/group__core__array.html#gad327659ac03e5fd6894b90025e6900a7) to obtain the coordinates of the object of the query image on the train image\n",
    "* Drawing lines on the train image that connect the coordinates of the object using [polylines](https://docs.opencv.org/3.4/d6/d6e/group__imgproc__draw.html#gaa3c25f9fb764b6bef791bf034f6e26f5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_coords = np.array([[0, 0], [img1.shape[1], 0], [img1.shape[1], img1.shape[0]], [0, img1.shape[0]]]).astype(np.float32).reshape(-1, 1, 2)\n",
    "\n",
    "h, _ = cv2.findHomography(query_pts, train_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "tar_coords = cv2.perspectiveTransform(corner_coords, h)\n",
    "\n",
    "# Draw the corners of the train image in the query image\n",
    "img3 = cv2.polylines(img2, [np.int32(tar_coords)], True, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "cv2.imshow('Warped Image', img3)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Segmentation\n",
    "\n",
    "### Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "img = cv2.imread(os.path.join(imagesDir, 'sudoku.png'))\n",
    "\n",
    "# Convert to grayscale\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otsu Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply global binary threshold\n",
    "ret, th_global = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Apply binary threshold with Otsu's method\n",
    "ret, th_otsu = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "# Show images\n",
    "cv2.imshow('Global Threshold', th_global)\n",
    "cv2.imshow('Otsu Threshold', th_otsu)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.1: Verify the effects of blurring the image using a Gaussian filter, before applying the Otsu thresholding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurred_img = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "# Apply global binary threshold\n",
    "ret, th_global = cv2.threshold(blurred_img, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Apply binary threshold with Otsu's method\n",
    "ret, th_otsu = cv2.threshold(blurred_img, 127, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "# Show images\n",
    "cv2.imshow('Global Threshold', th_global)\n",
    "cv2.imshow('Otsu Threshold', th_otsu)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply adaptive thresholding\n",
    "th_adaptive = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "# Show images\n",
    "cv2.imshow('Adaptive Mean', th_adaptive)\n",
    "cv2.imshow('Otsu Threshold', th_otsu)\n",
    "cv2.imshow('Global Threshold', th_global)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.2: Verify the effects of blurring with filters of increasing sizes before applying the adaptive threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [img, cv2.GaussianBlur(img, (5, 5), 0), cv2.GaussianBlur(img, (11, 11), 0), cv2.GaussianBlur(img, (15, 15), 0)]\n",
    "\n",
    "# Apply adaptive thresholding\n",
    "th_adaptive = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "# Show images\n",
    "cv2.imshow('No blur', th_adaptive)\n",
    "cv2.imshow('5x5 blur', cv2.adaptiveThreshold(imgs[1], 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2))\n",
    "cv2.imshow('11x11 blur', cv2.adaptiveThreshold(imgs[2], 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2))\n",
    "cv2.imshow('15x15 blur', cv2.adaptiveThreshold(imgs[3], 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation with [K-Means](https://docs.opencv.org/master/d5/d38/group__core__cluster.html#ga9a34dc06c6ec9460e90860f15bcd2f88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "img2 = cv2.imread(os.path.join(imagesDir, 'home.jpg'))\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape: (384, 512, 3)\n",
      "Current shape: (196608, 3)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the image and turn its values to float\n",
    "print(f\"Previous shape: {img2.shape}\")\n",
    "\n",
    "reshaped_image = img2.reshape((-1,3))\n",
    "reshaped_image = np.float32(reshaped_image)\n",
    "\n",
    "print(f\"Current shape: {reshaped_image.shape}\")\n",
    "\n",
    "# Define criteria and number of clusters (k)\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "k = 4\n",
    "\n",
    "# Apply K-means\n",
    "ret, label, center = cv2.kmeans(reshaped_image, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to uint8, and make resulting image\n",
    "center = np.uint8(center)\n",
    "result = center[label.flatten()]\n",
    "result = result.reshape((img2.shape))\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img2)\n",
    "cv2.imshow('K-Means Result', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.3: Experiment with different number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_image = img2.reshape((-1,3))\n",
    "reshaped_image = np.float32(reshaped_image)\n",
    "\n",
    "# Define criteria and number of clusters (k)\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "ks = [2, 4, 8, 16, 32]\n",
    "imgs = []\n",
    "\n",
    "for k in ks:\n",
    "    # Apply K-means\n",
    "    ret, label, center = cv2.kmeans(reshaped_image, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "    # Convert back to uint8, and make resulting image\n",
    "    center = np.uint8(center)\n",
    "    result = center[label.flatten()]\n",
    "    result = result.reshape((img2.shape))\n",
    "    imgs.append(result)\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img2)\n",
    "for i in range(len(imgs)):\n",
    "    cv2.imshow(f'K-Means Result with k={ks[i]}', imgs[i])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation with [GrabCut](https://docs.opencv.org/4.x/d3/d47/group__imgproc__segmentation.html#ga909c1dda50efcbeaa3ce126be862b37f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "img = cv2.imread(os.path.join(imagesDir, 'giraffe.jpg')) # Change this, according to your image's path\n",
    "\n",
    "# Resize image to facilitate visualization\n",
    "img = cv2.resize(img, (0, 0), fx = 0.6, fy = 0.6)\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image mask for the GrabCut output with same dimensions as the image\n",
    "mask = np.zeros(img.shape[:2], np.uint8)\n",
    "\n",
    "# Define the bounding box coordinates with the object of interest: (x, y, width, heigh)\n",
    "bb = (0, 0, 400, 500)\n",
    "\n",
    "# Allocate memory for the two arrays that this algorithm internally uses for the segmentation of the foreground and background\n",
    "bgModel = np.zeros((1, 65), np.float64)\n",
    "fgModel = np.zeros((1, 65), np.float64)\n",
    "\n",
    "# Apply GrabCut\n",
    "(mask, bgModel, fgModel) = cv2.grabCut(img, mask, bb, bgModel, fgModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "# All definite background and probable background pixels are set to 0, and all definite foreground and probable foreground pixels are set to 1\n",
    "output_mask = np.where((mask == cv2.GC_BGD) | (mask == cv2.GC_PR_BGD), 0, 1)\n",
    "\n",
    "# Scale the mask from the range [0, 1] to [0, 255]\n",
    "output_mask = (output_mask * 255).astype(\"uint8\")\n",
    "\n",
    "# Apply a bitwise AND to the image using the generated mask by GrabCut to obtain the final result\n",
    "grabcut_result = cv2.bitwise_and(img, img, mask=output_mask)\n",
    "\n",
    "# Show result\n",
    "cv2.imshow('Output Mask', output_mask)\n",
    "cv2.imshow('GrabCut Result', grabcut_result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.4: Select a region of interest in the image (using cv2.selectROI function) for the GrabCut algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n"
     ]
    }
   ],
   "source": [
    "# Define image mask for the GrabCut output with same dimensions as the image\n",
    "mask = np.zeros(img.shape[:2], np.uint8)\n",
    "\n",
    "# Define the bounding box coordinates with the object of interest: (x, y, width, heigh)\n",
    "roi = cv2.selectROI(img)\n",
    "\n",
    "# Allocate memory for the two arrays that this algorithm internally uses for the segmentation of the foreground and background\n",
    "bgModel = np.zeros((1, 65), np.float64)\n",
    "fgModel = np.zeros((1, 65), np.float64)\n",
    "\n",
    "# Apply GrabCut\n",
    "(mask, bgModel, fgModel) = cv2.grabCut(img, mask, roi, bgModel, fgModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "# All definite background and probable background pixels are set to 0, and all definite foreground and probable foreground pixels are set to 1\n",
    "output_mask = np.where((mask == cv2.GC_BGD) | (mask == cv2.GC_PR_BGD), 0, 1)\n",
    "\n",
    "# Scale the mask from the range [0, 1] to [0, 255]\n",
    "output_mask = (output_mask * 255).astype(\"uint8\")\n",
    "\n",
    "# Apply a bitwise AND to the image using the generated mask by GrabCut to obtain the final result\n",
    "grabcut_result = cv2.bitwise_and(img, img, mask=output_mask)\n",
    "\n",
    "# Show result\n",
    "cv2.imshow('Output Mask', output_mask)\n",
    "cv2.imshow('GrabCut Result', grabcut_result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
