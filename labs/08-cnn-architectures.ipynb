{"cells":[{"cell_type":"markdown","metadata":{"id":"oaU9_sOGzRD1"},"source":["# Lab 8: CNN Architectures\n","In this notebook we will explore standard CNN architectures using PyTorch and torchvision."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3454,"status":"ok","timestamp":1682417357457,"user":{"displayName":"Luís Teixeira","userId":"13735807174141480271"},"user_tz":-60},"id":"dvfTDUXuzRD9"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","np.random.seed(42)\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n","from sklearn.metrics import accuracy_score\n","from tqdm import tqdm\n","from torchsummary import summary"]},{"cell_type":"markdown","metadata":{"id":"BDBhCwhszREA"},"source":["### Loading CIFAR10 Dataset\n","\n","Since we are going to use networks that were pretrained on ImageNet, we need to normalize our data according to the mean and std with which these networks were trained, i.e. with the statistics of the ImageNet dataset."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11693,"status":"ok","timestamp":1682417369147,"user":{"displayName":"Luís Teixeira","userId":"13735807174141480271"},"user_tz":-60},"id":"w5IQVCmOzREB","outputId":"c37d8206-b754-4314-a8e0-80653b5d7685"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":[" 29%|██▉       | 49020928/170498071 [06:10<37:12, 54423.75it/s] "]}],"source":["# Normalize images\n","data_aug = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","  ])\n","\n","training_data = datasets.CIFAR10(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=data_aug,\n",")\n","validation_data = datasets.CIFAR10(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=data_aug,\n",")\n","print(f\"Training size: {len(training_data)} \\nValidation size: {len(validation_data)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1682417369148,"user":{"displayName":"Luís Teixeira","userId":"13735807174141480271"},"user_tz":-60},"id":"ByDvrH3z1uOT","outputId":"5c1f7007-2df3-4643-dc3f-ee5d8409f420"},"outputs":[],"source":["# get cpu or gpu device for training\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")\n","\n","# now we need to define a Dataloader, which allows us to automatically batch our inputs, do sampling and multiprocess data loading\n","batch_size = 64\n","num_workers = 2 # how many processes are used to load the data\n","\n","train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n","validation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)"]},{"cell_type":"markdown","metadata":{"id":"g1kRGiw_zREE"},"source":["### Training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1682417369148,"user":{"displayName":"Luís Teixeira","userId":"13735807174141480271"},"user_tz":-60},"id":"VxSdayviCWk5"},"outputs":[],"source":["def epoch_iter(dataloader, model, loss_fn, optimizer=None, is_train=True):\n","    if is_train:\n","      assert optimizer is not None, \"When training, please provide an optimizer.\"\n","      \n","    num_batches = len(dataloader)\n","\n","    if is_train:\n","      model.train() # put model in train mode\n","    else:\n","      model.eval()\n","\n","    total_loss = 0.0\n","    preds = []\n","    labels = []\n","\n","    with torch.set_grad_enabled(is_train):\n","      for batch, (X, y) in enumerate(tqdm(dataloader)):\n","          X, y = X.to(device), y.to(device)\n","\n","          # Compute prediction error\n","          pred = model(X)\n","          loss = loss_fn(pred, y)\n","\n","          if is_train:\n","            # Backpropagation\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","          # Save training metrics\n","          total_loss += loss.item() # IMPORTANT: call .item() to obtain the value of the loss WITHOUT the computational graph attached\n","\n","          probs = F.softmax(pred, dim=1)\n","          final_pred = torch.argmax(probs, dim=1)\n","          preds.extend(final_pred.cpu().numpy())\n","          labels.extend(y.cpu().numpy())\n","\n","    return total_loss / num_batches, accuracy_score(labels, preds)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1682417369148,"user":{"displayName":"Luís Teixeira","userId":"13735807174141480271"},"user_tz":-60},"id":"DmsUVGS6C0O1"},"outputs":[],"source":["def train(model, model_name, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer):\n","  train_history = {'loss': [], 'accuracy': []}\n","  val_history = {'loss': [], 'accuracy': []}\n","  best_val_loss = np.inf\n","  print(\"Start training...\")\n","  for t in range(num_epochs):\n","      print(f\"\\nEpoch {t+1}\")\n","      train_loss, train_acc = epoch_iter(train_dataloader, model, loss_fn, optimizer)\n","      print(f\"Train loss: {train_loss:.3f} \\t Train acc: {train_acc:.3f}\")\n","      val_loss, val_acc = epoch_iter(validation_dataloader, model, loss_fn, is_train=False)\n","      print(f\"Val loss: {val_loss:.3f} \\t Val acc: {val_acc:.3f}\")\n","\n","      # save model when val loss improves\n","      if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}\n","        torch.save(save_dict, model_name + '_best_model.pth')\n","\n","      # save latest model\n","      save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}\n","      torch.save(save_dict, model_name + '_latest_model.pth')\n","\n","      # save training history for plotting purposes\n","      train_history[\"loss\"].append(train_loss)\n","      train_history[\"accuracy\"].append(train_acc)\n","\n","      val_history[\"loss\"].append(val_loss)\n","      val_history[\"accuracy\"].append(val_acc)\n","      \n","  print(\"Finished\")\n","  return train_history, val_history"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1682417369149,"user":{"displayName":"Luís Teixeira","userId":"13735807174141480271"},"user_tz":-60},"id":"xr48TEVlzREH"},"outputs":[],"source":["# Plot loss and accuracy during training\n","def plotTrainingHistory(train_history, val_history):\n","    plt.subplot(2, 1, 1)\n","    plt.title('Cross Entropy Loss')\n","    plt.plot(train_history['loss'], label='train')\n","    plt.plot(val_history['loss'], label='val')\n","    plt.legend(loc='best')\n","\n","    plt.subplot(2, 1, 2)\n","    plt.title('Classification Accuracy')\n","    plt.plot(train_history['accuracy'], label='train')\n","    plt.plot(val_history['accuracy'], label='val')\n","\n","    plt.tight_layout()\n","    plt.legend(loc='best')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Cc3lLW4KQlDc"},"source":["### Custom CNN\n","\n","Adapt custom CNN from last week to CIFAR10 data (32x32 colored images as input)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6142,"status":"ok","timestamp":1682417375274,"user":{"displayName":"Luís Teixeira","userId":"13735807174141480271"},"user_tz":-60},"id":"1pD9QVgDQL0q","outputId":"bdf5ea7e-8650-4ff5-8a77-1d86295c2757"},"outputs":[],"source":["class ConvolutionalNeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(ConvolutionalNeuralNetwork, self).__init__()\n","        # TODO\n","\n","    def forward(self, x):\n","        logits = self.layers(x)\n","        return logits\n","\n","custom_cnn = ConvolutionalNeuralNetwork()\n","\n","custom_cnn.to(device)\n","print(custom_cnn)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":221224,"status":"ok","timestamp":1682417596485,"user":{"displayName":"Luís Teixeira","userId":"13735807174141480271"},"user_tz":-60},"id":"F2MjpYwWQL0r","outputId":"8c6cf9b0-c9f3-47dd-de4a-4342ceb8723b"},"outputs":[],"source":["# Train network for 10 epochs\n","num_epochs = 10\n","model_name = 'custom CNN'\n","\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer_custom = torch.optim.SGD(custom_cnn.parameters(), lr=1e-3)\n","\n","custom_train_history, custom_val_history = train(custom_cnn, model_name, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer_custom)\n","\n","plotTrainingHistory(custom_train_history, custom_val_history)"]},{"cell_type":"markdown","metadata":{"id":"cZSe3Knytp5A"},"source":["### 7x7 versus 3x3 Convolutions\n","\n","Historically, the VGG was the first CNN architecture to introduce more layers (16-19 layers versus the 8 layers of AlexNet) and smaller convolutional kernel sizes (3x3).\n","\n","This is mainly due to the fact that a stack of 3 3x3 convolutional layers (with stride 1) has the same effective receptive field as a single 7x7 layer. Why?\n","\n","You can find out more about the effective receptive field of CNNs and explore some visualizations [here](https://blog.mlreview.com/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807).\n","\n","Compare the number of parameters of a single convolutional layer with 7x7 kernel with a stack of 3 convolutional layers with 3x3 kernels."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682417596486,"user":{"displayName":"Luís Teixeira","userId":"13735807174141480271"},"user_tz":-60},"id":"6hg9leFzuRf3","outputId":"9e5fe35b-e438-4456-e863-a9b4388bed6b"},"outputs":[],"source":["# TODO\n","# Use the summary method from the torchsummary package\n","# Compare one 7x7 convolution layer with three 3x3 layers\n","# Consider layers with only 1 input and output channels"]},{"cell_type":"markdown","metadata":{"id":"7Lc1auKjyMfr"},"source":["What are the advantages of using a stack of 3 convolutional layers with 3x3 kernels instead of a single 7x7 layer?"]},{"cell_type":"markdown","metadata":{"id":"0aqWR_0VzRED"},"source":["## Pre-trained Models - VGG\n","\n","[Click here to check the pre-trained models that are available on torchvision.](https://pytorch.org/vision/0.9/models.html)\n","\n","\n","Load and adapt a VGG-16 model for the CIFAR10 data, through the following steps:\n","* Load the pre-trained VGG-16 model from torchvision\n","* Alter the output layer of the network to match the number of classes of the CIFAR10 data (10 classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5017,"status":"ok","timestamp":1682417601498,"user":{"displayName":"Luís Teixeira","userId":"13735807174141480271"},"user_tz":-60},"id":"DfK3c9RSzRED","outputId":"8ae04c00-4a82-433e-fca2-e962aa369774"},"outputs":[],"source":["# Load VGG model from torchvision (with pretrained=True)\n","vgg = # TODO\n","\n","# Change the number of neurons in the last layer to the number of classes of the CIFAR10 dataset\n","# TODO\n","\n","vgg.to(device)\n","print(vgg)"]},{"cell_type":"markdown","metadata":{"id":"JX-vNZF3fiqs"},"source":["Inspect the model structure.\n","\n","What does the AdaptiveAvgPool2d layer do?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"aAl5WoxWlwcK"},"outputs":[],"source":["# Train network for 10 epochs\n","num_epochs = 10\n","model_name = 'vgg16'\n","\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer_vgg = torch.optim.SGD(vgg.parameters(), lr=1e-3)\n","\n","vgg_train_history, vgg_val_history = train(vgg, model_name, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer_vgg)\n","\n","plotTrainingHistory(vgg_train_history, vgg_val_history)"]},{"cell_type":"markdown","metadata":{"id":"aDzxzS63q4AP"},"source":["## Pre-trained Models - ResNet\n","\n","[Click here to check the pre-trained models that are available on torchvision.](https://pytorch.org/vision/0.9/models.html)\n","\n","\n","Load and adapt a ResNet-50 model for the CIFAR10 data, through the following steps:\n","* Load the pre-trained VGG-50 model from torchvision\n","* Alter the output layer of the network to match the number of classes of the CIFAR10 data (10 classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MTZd8yFhrL9b"},"outputs":[],"source":["# Load ResNet model from torchvision (with pretrained=True)\n","resnet = # TODO\n","\n","# Change the number of neurons in the last layer to the number of classes of the CIFAR10 dataset\n","# TODO\n","\n","resnet.to(device)\n","print(resnet)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oMlKInxPrgS-"},"outputs":[],"source":["# Train network for 10 epochs\n","num_epochs = 10\n","model_name = 'resnet50'\n","\n","loss_fn = nn.CrossEntropyLoss() # already includes the Softmax activation\n","optimizer_resnet = torch.optim.SGD(resnet.parameters(), lr=1e-3)\n","\n","resnet_train_history, resnet_val_history = train(resnet, model_name, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer_resnet)\n","\n","plotTrainingHistory(resnet_train_history, resnet_val_history)"]},{"cell_type":"markdown","metadata":{"id":"u6pf9u38zADi"},"source":["### Compare the VGG and ResNet models in terms of:\n","\n","\n","1.   number of parameters\n","2.   validation accuracy\n","3.   training time\n","\n","What is the main difference introduced by the ResNet architecture?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fEtXUkArZx8J"},"outputs":[],"source":["# TODO: use torchsummary to compare VGG-16 and ResNet-50"]},{"cell_type":"markdown","metadata":{"id":"6Ry-OGmeQL0v"},"source":["## Challenges\n","\n","**Challenge 1:** In transfer learning, we often replace the head of the model (fully-connected layers responsible for classification) to fit the task. However, these new layers are not pre-trained and thus they contain an error that is backpropagated through the pre-trained part of the network during training. We can avoid this through a training strategy that is divided into two steps:\n","* Freeze the pre-trained layers of the network so that their parameters are no longer updated during training and train only the head of the model\n","* Unfreeze these layers and train the network as a whole.\n","Implement this strategy and see the results!\n","\n","**Challenge 2:** Experiment with other CNN architectures available on torchvision."]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
