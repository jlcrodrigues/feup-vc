{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Introduction to OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this first lab is to present a small introduction to image processing using OpenCV. In each section, you can find:\n",
    "* a small example - analyse the code and try it\n",
    "* some exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements for this tutorial\n",
    "! pip install opencv-python\n",
    "! pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you prefer, you can convert this notebook to a Python script by uncommenting the following command\n",
    "! pip install nbconvert\n",
    "! jupyter nbconvert --to script 01-introduction.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dataDir = './data'\n",
    "path=\"ruben.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Images – read, write and display; ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/jose.l.rodrigues/.local/lib/python3.10/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "# Opening an image\n",
    "img = cv2.imread(os.path.join(dataDir, 'ml.jpg'))\n",
    "\n",
    "# Showing the image\n",
    "cv2.imshow(\"ml.jpg\", img)\n",
    "\n",
    "# Waiting for user to press a key to close the image\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Close the window after user pressed a key\n",
    "cv2.destroyWindow(\"ml.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height: 380\n",
      "width: 308\n",
      "channels: 3\n"
     ]
    }
   ],
   "source": [
    "# Check image size\n",
    "h, w, c = img.shape\n",
    "print(f'height: {h}')\n",
    "print(f'width: {w}')\n",
    "print(f'channels: {c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving image in bmp format\n",
    "cv2.imwrite('ml_new.bmp', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.1 - Read any other color image from a file, show the mouse cursor over the image, and the coordinates and RGB components of the pixel under the cursor. When the user clicks on the mouse, let him modify the RGB components of the selected pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 104, y: 148\n",
      "RGB: [141 162 207]\n"
     ]
    }
   ],
   "source": [
    "#Exercise 1.1 - Read any other color image from a file, show the mouse cursor over the image, and the coordinates and RGB components of the pixel under the cursor. When the user clicks on the mouse, let him modify the RGB components of the selected pixel\n",
    "\n",
    "# Opening an image\n",
    "img = cv2.imread(os.path.join(dataDir, path))\n",
    "\n",
    "# Showing the image\n",
    "cv2.imshow(path, img)\n",
    "\n",
    "\n",
    "square_size=6\n",
    "color=[0, 0, 255]\n",
    "# Mouse callback function\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        print(f'x: {x}, y: {y}')\n",
    "        print(f'RGB: {img[y, x]}')\n",
    "        for i in range(square_size):\n",
    "            for j in range(square_size):\n",
    "                img[y + i, x + j] = color\n",
    "        cv2.imshow(path, img)\n",
    "\n",
    "# Set the callback function for any mouse event\n",
    "cv2.setMouseCallback(path, mouse_callback)\n",
    "\n",
    "# Waiting for user to press a key to close the image\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Close the window after user pressed a key\n",
    "cv2.destroyWindow(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.2 - Allow the user to select a region of interest (ROI) in the image, by clicking on two points that identify two opposite corners of the selected ROI, and save the ROI into another file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n",
      "(49, 21, 79, 114)\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(os.path.join(dataDir, path))\n",
    "cv2.imshow(path, img)\n",
    "\n",
    "roi = cv2.selectROI(path, img, fromCenter=False, showCrosshair=True)\n",
    "with open('roi.txt', 'w') as f:\n",
    "    print(roi)\n",
    "    f.write(f'{roi[0]} {roi[1]} {roi[2]} {roi[3]}')\n",
    "    cv2.imwrite(f\"{path}_roi.jpg\", img[roi[1]:roi[1]+roi[3], roi[0]:roi[0]+roi[2]])\n",
    "\n",
    "# Waiting for user to press a key to close the image\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Close the window after user pressed a key\n",
    "cv2.destroyWindow(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Images – representation, grayscale and color, color spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a white image\n",
    "m = np.ones((100,200,1), np.uint8)\n",
    "\n",
    "# Change the intensity to 100\n",
    "m = m * 100\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow('Grayscale image', m)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow('Grayscale image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a line with thickness of 5 px\n",
    "cv2.line(m, (0,0), (200,100), 255, 5)\n",
    "cv2.line(m, (200, 0), (0, 100), 255, 5)\n",
    "cv2.imshow('Grayscale image with diagonals', m)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow('Grayscale image with diagonals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.1 - Create a color image with 100(lines)x200(columns) pixels with yellow color; draw the two diagonals of the image, one in red color, the other in blue color. Display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yellow matrix\n",
    "m = np.ones((100, 200, 3), np.uint8)\n",
    "m[:, :, 0] = 0\n",
    "m[:, :, 1] = 255\n",
    "m[:, :, 2] = 255\n",
    "\n",
    "cv2.line(m, (0, 0), (200, 100), (255, 0, 0), 5)\n",
    "cv2.line(m, (200, 0), (0, 100), (0, 0, 255), 5)\n",
    "cv2.imshow('Color image with diagonals', m)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow('Color image with diagonals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.2 - Read any color image, in RGB format, display it in one window, convert it to grayscale, display the grayscale image in another window and save the grayscale image to a different file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(os.path.join(dataDir, path))\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('Grayscale image', gray)\n",
    "cv2.imshow('Color image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow('Grayscale image')\n",
    "cv2.destroyWindow('Color image')\n",
    "cv2.imwrite('gray.jpg', gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.3 - Split the 3 RGB channels and show each channel in a separate window. Add a constant value to one of the channels, merge the channels into a new color image and show the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join(dataDir, path))\n",
    "#red = img[:, :, 2]\n",
    "red = img.copy()\n",
    "red[:, :, 0] = 0\n",
    "red[:, :, 1] = 0\n",
    "cv2.imshow('Rubinho Vermelhinho', red)\n",
    "\n",
    "green = img.copy()\n",
    "green[:, :, 0] = 0\n",
    "green[:, :, 2] = 0\n",
    "cv2.imshow('Rubinho verdinho', green)\n",
    "\n",
    "blue = img.copy()\n",
    "blue[:, :, 1] = 0\n",
    "blue[:, :, 2] = 0\n",
    "cv2.imshow('Rubinho blue', blue)\n",
    "\n",
    "merge = cv2.merge((blue[:, :, 0] + 50, green[:, :, 1], red[:, :, 2]))\n",
    "cv2.imshow('Rubinho multicolorz', merge)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.4 - Convert the image to HSV, split the 3 HSV channels and show each channel in a separate window. Add a constant value to saturation channel, merge the channels into a new color image and show the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Video – acquisition and simple processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m frame_nr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Capture frame-by-frame\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# If frame is read correctly ret is True\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define a VideoCapture Object\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "frame_nr = 0\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # If frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('webcam', frame)\n",
    "\n",
    "    # Wait for user to press s to save frame\n",
    "    if cv2.waitKey(1) == ord('s'):\n",
    "        frame_name = 'frame' + str(frame_nr) + '.png'\n",
    "        cv2.imwrite(frame_name, frame)\n",
    "        cv2.imshow(\"Saved frame: \" + frame_name, frame)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyWindow(\"Saved frame: \" + frame_name)\n",
    "\n",
    "    # Wait for user to press q to quit\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_nr += 1\n",
    "\n",
    "# When everything is done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.1 - Using the previous example as the baseline, implement a script that acquires the video from the webcam, converts it to grayscale, and shows the frames in binary format (i.e. the intensity of each pixel is 0 or 255); use a threshold value of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/jose.l.rodrigues/.local/lib/python3.10/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "# Define a VideoCapture Object\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "frame_nr = 0\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # If frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "        #convert to grayscale\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    _, frame = cv2.threshold(frame, 100, 255, cv2.THRESH_BINARY)\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('webcam', frame)\n",
    "\n",
    "    # Wait for user to press s to save frame\n",
    "    if cv2.waitKey(1) == ord('s'):\n",
    "        frame_name = 'frame' + str(frame_nr) + '.png'\n",
    "        cv2.imwrite(frame_name, frame)\n",
    "        cv2.imshow(\"Saved frame: \" + frame_name, frame)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyWindow(\"Saved frame: \" + frame_name)\n",
    "\n",
    "    # Wait for user to press q to quit\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_nr += 1\n",
    "\n",
    "# When everything is done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.2 - Implement a simple detection/tracking algorithm for colored objects, using the following steps:\n",
    "1) take each frame of the video;\n",
    "2) convert from BGR to HSV color-space;\n",
    "3) threshold the HSV image for a range of color values (creating a binary image);\n",
    "4) extract the objects of the selected range (with a bitwise AND operation, using as operands the original and the binary image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "frame_nr = 0\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # If frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    #convert to hsv\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    lower = np.array([90, 100, 0])\n",
    "    upper = np.array([160, 255, 255])\n",
    "    mask = cv2.inRange(frame, lower, upper)\n",
    "    frame = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('webcam', frame)\n",
    "\n",
    "    # Wait for user to press s to save frame\n",
    "    if cv2.waitKey(1) == ord('s'):\n",
    "        frame_name = 'frame' + str(frame_nr) + '.png'\n",
    "        cv2.imwrite(frame_name, frame)\n",
    "        cv2.imshow(\"Saved frame: \" + frame_name, frame)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyWindow(\"Saved frame: \" + frame_name)\n",
    "\n",
    "    # Wait for user to press q to quit\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_nr += 1\n",
    "\n",
    "# When everything is done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
